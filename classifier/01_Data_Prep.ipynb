{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "\n",
    "import wordsegment as ws\n",
    "ws.load()\n",
    "\n",
    "from code.util import *\n",
    "from code.preprocessor import *\n",
    "from code.gofaster import *\n",
    "\n",
    "gf = GoFaster(11, 22)\n",
    "\n",
    "N0 = 45000\n",
    "N1 = 15000\n",
    "VOCAB = 5000\n",
    "CVECT = 100\n",
    "WVECT = 20\n",
    "UCHARS = 139\n",
    "MI = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0 = pd.read_csv(\"data/raw/url_0.csv\", skiprows=0).sample(\n",
    "    n=N0,\n",
    "    random_state=11\n",
    ").reset_index(drop=True)\n",
    "\n",
    "url1 = pd.read_csv(\"data/raw/url_1.csv\", skiprows=0).sample(\n",
    "    n=N1,\n",
    "    random_state=11\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0[\"target\"] = pd.Series(np.zeros(N0).astype(np.int32))\n",
    "url1[\"target\"] = pd.Series(np.ones(N1).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = pd.concat(\n",
    "    [\n",
    "        url0[[\"url\", \"target\"]],\n",
    "        url1[[\"url\", \"target\"]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(url, \"data/url_45_15.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x0 = url.url\n",
    "y0 = url.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Target Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_onehot = to_categorical(y0, dtype=np.int32)\n",
    "np.save(\"data/xy/y_onehot_45_15.npy\", y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(\n",
    "    num_words=VOCAB,\n",
    "    filters='!\"#$%&()*+,-.:;<=>?@[\\\\]^_`{|}~',\n",
    "    split=\"/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.fit_on_texts(x0)\n",
    "x_word = pad_sequences(tk.texts_to_sequences(x0), maxlen=WVECT)\n",
    "x_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/xy/x_word_45_15.npy\", x_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = load(\"data/url_45_15.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_char = np.vstack(url.url.apply(char_level_encoder, args=(CVECT,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104, 116, 116, ...,   0,   0,   0],\n",
       "       [104, 116, 116, ...,   0,   0,   0],\n",
       "       [104, 116, 116, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [104, 116, 116, ..., 117, 115, 116],\n",
       "       [104, 116, 116, ...,   0,   0,   0],\n",
       "       [104, 116, 116, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = np.unique(x_char).astype(int)\n",
    "len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 13900)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_char_onehot = char_onehot(x_char, unique_chars, UCHARS)\n",
    "x_char_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x_char_onehot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"data/xy/x_char_onehot_45_15.npz\", x_char_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\"data/url_45_15.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = load(\"data/mi_100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_tlds = ['country','stream','download','xin','gdn','racing', \n",
    "                   'jetzt','win','bid','vip', 'ren', 'kim', 'loan',\n",
    "                   'mom', 'party', 'review', 'trade', 'date', 'wang',\n",
    "                   'accountants', 'zip','cricket','link','work','gq',\n",
    "                   'science','tk', 'world', 'fit', 'work' 'ryukyu',\n",
    "                   'life', 'cloud', 'desi', 'okinawa', 'ooo','men',\n",
    "                   'click', 'loan', 'top', 'cf', 'ml', 'ga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data.url.apply(tokenize)\n",
    "data[\"len_url\"] = data.url.apply(len)\n",
    "data[\"len_tok\"] = data.tokens.apply(len)\n",
    "data[\"slash\"] = data.url.apply(lambda url: url.count(\"/\"))\n",
    "data[\"dbl_slash\"] = data.url.apply(lambda url: url.count(\"//\"))\n",
    "data[\"dot\"] = data.url.apply(lambda url: url.count(\".\"))\n",
    "data[\"at\"] = data.url.apply(lambda url: url.count(\"@\"))\n",
    "data[\"dash\"] = data.url.apply(lambda url: url.count(\"-\"))\n",
    "data[\"qmark\"] = data.url.apply(lambda url: url.count(\"?\"))\n",
    "data[\"amp\"] = data.url.apply(lambda url: url.count(\"&\"))\n",
    "data[\"hash\"] = data.url.apply(lambda url: url.count(\"#\"))\n",
    "data[\"perc\"] = data.url.apply(lambda url: url.count(\"%\"))\n",
    "data[\"eq\"] = data.url.apply(lambda url: url.count(\"=\"))\n",
    "data[\"colon\"] = data.url.apply(lambda url: url.count(\":\"))\n",
    "data[\"scolon\"] = data.url.apply(lambda url: url.count(\";\"))\n",
    "data[\"scheme\"] = data.url.apply(lambda url: urlparse(url).scheme)\n",
    "data[\"netloc\"] = data.url.apply(lambda url: urlparse(url).netloc)\n",
    "data[\"tld\"] = data.netloc.apply(lambda netloc: extract(netloc).suffix)\n",
    "data[\"suspicious\"] = data.tld.apply(lambda tld: 1 if tld in suspicious_tlds else 0)\n",
    "\n",
    "def non_ascii(url):\n",
    "    n = 0\n",
    "    \n",
    "    for c in url:\n",
    "        if ord(c) > 127:\n",
    "            n += 1\n",
    "            \n",
    "    return n\n",
    "\n",
    "data[\"non_ascii\"] = data.url.apply(non_ascii)\n",
    "\n",
    "for i in range(MI):\n",
    "    data[\"mi%d\" % (i)] = data.tokens.apply(lambda tokens: 1 if words[i] in tokens else 0)\n",
    "    \n",
    "dummies = pd.get_dummies(data.scheme, drop_first=True)\n",
    "data = pd.concat([data, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [\n",
    "    \"target\",\n",
    "    \"url\",\n",
    "    \"tokens\",\n",
    "    \"scheme\",\n",
    "    \"netloc\",\n",
    "    \"tld\"\n",
    "]\n",
    "\n",
    "data = data.drop(columns=drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/xy/x_feat_45_15.npy\", data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\"data/url_45_15.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(tokens):\n",
    "    segments = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(\" \".join(ws.segment(token)))\n",
    "        \n",
    "    return \" \".join(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(df):\n",
    "    df[\"segments\"] = df.tokens.apply(segment)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data.url.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gf.parallelize(data, worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[http, www, meigaoyi, com]</td>\n",
       "      <td>http www mei gao yi com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[http, www, cybertruffle, org, uk, vinales, en...</td>\n",
       "      <td>http www cyber truffle org uk vina les eng sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[http, prague, tv, funny, pictures, archive]</td>\n",
       "      <td>http prague tv funny pictures archive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[http, www, f1autographs, com]</td>\n",
       "      <td>http www f1 autographs com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[http, members3, boardhost, com, ratterriers]</td>\n",
       "      <td>http members 3 boardhost com rat terriers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[http, octts, com]</td>\n",
       "      <td>http oct ts com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[http, www, consortiuminfo, org]</td>\n",
       "      <td>http www consortium info org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[http, www, hmbreview, com]</td>\n",
       "      <td>http www hmb review com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[http, www, slopezphotography, com]</td>\n",
       "      <td>http www s lopez photography com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[http, www, metacritic, com, video, titles, ti...</td>\n",
       "      <td>http www metacritic com video titles timecode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[http, mindside19, iwarp, com]</td>\n",
       "      <td>http mind side 19 i warp com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[http, www, cs, arizona, edu, patterns, weavin...</td>\n",
       "      <td>http www cs arizona edu patterns weaving books...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[http, www, vineyard, graphics, com]</td>\n",
       "      <td>http www vineyard graphics com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[http, www, angelfire, com, ny3, gokusrevenge,...</td>\n",
       "      <td>http www angelfire com ny3 goku s revenge fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[http, www, abacus, ca]</td>\n",
       "      <td>http www abacus ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[http, community, livejournal, com, be, the, p...</td>\n",
       "      <td>http community livejournal com be the pinecone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[http, www, personal, umich, edu, pfa]</td>\n",
       "      <td>http www personal umich edu pfa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[http, www, htamler, com]</td>\n",
       "      <td>http www ht am ler com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[http, triviaspot, com, trivia, trivia16, asp]</td>\n",
       "      <td>http trivia spot com trivia trivia 16 asp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[http, www, sonypictures, com, movies, badboys2]</td>\n",
       "      <td>http www sony pictures com movies bad boys 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[http, www, elomnibus, com]</td>\n",
       "      <td>http www el omnibus com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[http, www, ajreeves, com]</td>\n",
       "      <td>http www aj reeves com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[http, www, cookitsimply, com, fish, and, seaf...</td>\n",
       "      <td>http www cook it simply com fish and seafood h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[http, www, angelfire, com, sk, hansondome]</td>\n",
       "      <td>http www angelfire com sk hanson dome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[http, groups, yahoo, com, group, toulaarp]</td>\n",
       "      <td>http groups yahoo com group to ul aarp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[http, www, geima, it, aziendaen, htm]</td>\n",
       "      <td>http www ge ima it azienda en htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[http, www, allmusic, com, cg, amg, dll, p, am...</td>\n",
       "      <td>http www all music com cg amg dll p amg amp sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[http, www, csd, uwo, ca, infocom, bureaucracy...</td>\n",
       "      <td>http www csd uwo ca infocom bureaucracy html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[http, franconia, to]</td>\n",
       "      <td>http franconia to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[http, pagestream, org]</td>\n",
       "      <td>http page stream org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokens  \\\n",
       "0                          [http, www, meigaoyi, com]   \n",
       "1   [http, www, cybertruffle, org, uk, vinales, en...   \n",
       "2        [http, prague, tv, funny, pictures, archive]   \n",
       "3                      [http, www, f1autographs, com]   \n",
       "4       [http, members3, boardhost, com, ratterriers]   \n",
       "5                                  [http, octts, com]   \n",
       "6                    [http, www, consortiuminfo, org]   \n",
       "7                         [http, www, hmbreview, com]   \n",
       "8                 [http, www, slopezphotography, com]   \n",
       "9   [http, www, metacritic, com, video, titles, ti...   \n",
       "10                     [http, mindside19, iwarp, com]   \n",
       "11  [http, www, cs, arizona, edu, patterns, weavin...   \n",
       "12               [http, www, vineyard, graphics, com]   \n",
       "13  [http, www, angelfire, com, ny3, gokusrevenge,...   \n",
       "14                            [http, www, abacus, ca]   \n",
       "15  [http, community, livejournal, com, be, the, p...   \n",
       "16             [http, www, personal, umich, edu, pfa]   \n",
       "17                          [http, www, htamler, com]   \n",
       "18     [http, triviaspot, com, trivia, trivia16, asp]   \n",
       "19   [http, www, sonypictures, com, movies, badboys2]   \n",
       "20                        [http, www, elomnibus, com]   \n",
       "21                         [http, www, ajreeves, com]   \n",
       "22  [http, www, cookitsimply, com, fish, and, seaf...   \n",
       "23        [http, www, angelfire, com, sk, hansondome]   \n",
       "24        [http, groups, yahoo, com, group, toulaarp]   \n",
       "25             [http, www, geima, it, aziendaen, htm]   \n",
       "26  [http, www, allmusic, com, cg, amg, dll, p, am...   \n",
       "27  [http, www, csd, uwo, ca, infocom, bureaucracy...   \n",
       "28                              [http, franconia, to]   \n",
       "29                            [http, pagestream, org]   \n",
       "\n",
       "                                             segments  \n",
       "0                             http www mei gao yi com  \n",
       "1   http www cyber truffle org uk vina les eng sch...  \n",
       "2               http prague tv funny pictures archive  \n",
       "3                          http www f1 autographs com  \n",
       "4           http members 3 boardhost com rat terriers  \n",
       "5                                     http oct ts com  \n",
       "6                        http www consortium info org  \n",
       "7                             http www hmb review com  \n",
       "8                    http www s lopez photography com  \n",
       "9       http www metacritic com video titles timecode  \n",
       "10                       http mind side 19 i warp com  \n",
       "11  http www cs arizona edu patterns weaving books...  \n",
       "12                     http www vineyard graphics com  \n",
       "13  http www angelfire com ny3 goku s revenge fram...  \n",
       "14                                 http www abacus ca  \n",
       "15     http community livejournal com be the pinecone  \n",
       "16                    http www personal umich edu pfa  \n",
       "17                             http www ht am ler com  \n",
       "18          http trivia spot com trivia trivia 16 asp  \n",
       "19       http www sony pictures com movies bad boys 2  \n",
       "20                            http www el omnibus com  \n",
       "21                             http www aj reeves com  \n",
       "22  http www cook it simply com fish and seafood h...  \n",
       "23              http www angelfire com sk hanson dome  \n",
       "24             http groups yahoo com group to ul aarp  \n",
       "25                  http www ge ima it azienda en htm  \n",
       "26  http www all music com cg amg dll p amg amp sq...  \n",
       "27       http www csd uwo ca infocom bureaucracy html  \n",
       "28                                  http franconia to  \n",
       "29                               http page stream org  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"tokens\", \"segments\"]].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(\n",
    "    num_words=VOCAB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.fit_on_texts(data.segments)\n",
    "x_segments = pad_sequences(tk.texts_to_sequences(data.segments), maxlen=WVECT)\n",
    "x_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/xy/x_segments_45_15.npy\", x_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
